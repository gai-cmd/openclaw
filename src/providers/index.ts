import Anthropic from '@anthropic-ai/sdk';
import OpenAI from 'openai';
import { GoogleGenerativeAI } from '@google/generative-ai';
import { config, type AgentType } from '../config.js';
import { logger } from '../utils/logger.js';

export type ProviderType = 'anthropic' | 'openai' | 'gemini';

export interface AgentModelConfig {
  provider: ProviderType;
  model: string;
}

// ì—ì´ì „íŠ¸ë³„ ëª¨ë¸ ì„¤ì •
export const AGENT_MODELS: Record<AgentType, AgentModelConfig> = {
  po: {
    provider: (config.PO_PROVIDER as ProviderType) || 'anthropic',
    model: config.PO_MODEL || 'claude-sonnet-4-5-20250929',
  },
  dev: {
    provider: (config.DEV_PROVIDER as ProviderType) || 'anthropic',
    model: config.DEV_MODEL || 'claude-sonnet-4-5-20250929',
  },
  design: {
    provider: (config.DESIGN_PROVIDER as ProviderType) || 'gemini',
    model: config.DESIGN_MODEL || 'gemini-2.0-flash',
  },
  cs: {
    provider: (config.CS_PROVIDER as ProviderType) || 'openai',
    model: config.CS_MODEL || 'gpt-4o',
  },
  marketing: {
    provider: (config.MARKETING_PROVIDER as ProviderType) || 'openai',
    model: config.MARKETING_MODEL || 'gpt-4o',
  },
};

// PO ë¹ ë¥¸ ì‘ë‹µ ëª¨ë¸ (Haiku)
export const PO_FAST_MODEL = config.PO_FAST_MODEL || 'claude-haiku-4-5-20251001';

// ============================================================
// í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  ì‹œìŠ¤í…œ
// ============================================================

interface TokenUsageEntry {
  inputTokens: number;
  outputTokens: number;
  calls: number;
}

interface DailyTokenUsage {
  date: string; // YYYY-MM-DD
  byProvider: Record<ProviderType, Record<string, TokenUsageEntry>>; // provider â†’ model â†’ usage
  byAgent: Record<string, TokenUsageEntry>; // agentType â†’ usage
}

let dailyUsage: DailyTokenUsage = createEmptyDailyUsage();

function createEmptyDailyUsage(): DailyTokenUsage {
  return {
    date: new Date().toISOString().split('T')[0],
    byProvider: {
      anthropic: {},
      openai: {},
      gemini: {},
    },
    byAgent: {},
  };
}

function ensureCurrentDay() {
  const today = new Date().toISOString().split('T')[0];
  if (dailyUsage.date !== today) {
    // ë‚ ì§œ ë³€ê²½ â†’ ë¦¬ì…‹
    dailyUsage = createEmptyDailyUsage();
  }
}

export function trackTokenUsage(
  provider: ProviderType,
  model: string,
  agentType: string,
  inputTokens: number,
  outputTokens: number
) {
  ensureCurrentDay();

  // í”„ë¡œë°”ì´ë”ë³„ ëª¨ë¸ë³„ ì¶”ì 
  if (!dailyUsage.byProvider[provider][model]) {
    dailyUsage.byProvider[provider][model] = { inputTokens: 0, outputTokens: 0, calls: 0 };
  }
  dailyUsage.byProvider[provider][model].inputTokens += inputTokens;
  dailyUsage.byProvider[provider][model].outputTokens += outputTokens;
  dailyUsage.byProvider[provider][model].calls += 1;

  // ì—ì´ì „íŠ¸ë³„ ì¶”ì 
  if (!dailyUsage.byAgent[agentType]) {
    dailyUsage.byAgent[agentType] = { inputTokens: 0, outputTokens: 0, calls: 0 };
  }
  dailyUsage.byAgent[agentType].inputTokens += inputTokens;
  dailyUsage.byAgent[agentType].outputTokens += outputTokens;
  dailyUsage.byAgent[agentType].calls += 1;

  logger.info('TOKENS', `${provider}/${model} [${agentType}] in:${inputTokens} out:${outputTokens}`);
}

// ì¼ì¼ í† í° ì‚¬ìš©ëŸ‰ ë¦¬í¬íŠ¸ ìƒì„± (POê°€ í˜¸ì¶œ)
export function getTokenUsageReport(): string {
  ensureCurrentDay();

  const lines: string[] = [];
  lines.push(`ğŸ“Š <b>í† í° ì‚¬ìš©ëŸ‰ ë¦¬í¬íŠ¸</b> (${dailyUsage.date})`);
  lines.push('');

  // í”„ë¡œë°”ì´ë”ë³„
  lines.push('<b>â–¸ í”„ë¡œë°”ì´ë”ë³„ ì‚¬ìš©ëŸ‰</b>');
  for (const [provider, models] of Object.entries(dailyUsage.byProvider)) {
    const modelEntries = Object.entries(models);
    if (modelEntries.length === 0) continue;

    let providerTotal = { input: 0, output: 0, calls: 0 };
    for (const [model, usage] of modelEntries) {
      providerTotal.input += usage.inputTokens;
      providerTotal.output += usage.outputTokens;
      providerTotal.calls += usage.calls;
      lines.push(`  ${provider}/${model}: ${usage.calls}íšŒ, in:${usage.inputTokens.toLocaleString()} / out:${usage.outputTokens.toLocaleString()}`);
    }
    lines.push(`  <b>${provider} ì†Œê³„</b>: ${providerTotal.calls}íšŒ, in:${providerTotal.input.toLocaleString()} / out:${providerTotal.output.toLocaleString()}`);
    lines.push('');
  }

  // ì—ì´ì „íŠ¸ë³„
  lines.push('<b>â–¸ ì—ì´ì „íŠ¸ë³„ ì‚¬ìš©ëŸ‰</b>');
  const agentNames: Record<string, string> = {
    po: 'ì´ë ˆ(PO)', dev: 'ë‹¤ì˜¨(Dev)', design: 'ì±„ì•„(Design)',
    cs: 'ë‚˜ë˜(CS)', marketing: 'ì•Œë¦¬(Marketing)',
  };
  let grandTotal = { input: 0, output: 0, calls: 0 };
  for (const [agent, usage] of Object.entries(dailyUsage.byAgent)) {
    const name = agentNames[agent] || agent;
    lines.push(`  ${name}: ${usage.calls}íšŒ, in:${usage.inputTokens.toLocaleString()} / out:${usage.outputTokens.toLocaleString()}`);
    grandTotal.input += usage.inputTokens;
    grandTotal.output += usage.outputTokens;
    grandTotal.calls += usage.calls;
  }
  lines.push('');
  lines.push(`<b>â–¸ ì´í•©</b>: ${grandTotal.calls}íšŒ, in:${grandTotal.input.toLocaleString()} / out:${grandTotal.output.toLocaleString()}`);

  return lines.join('\n');
}

// í† í° ì‚¬ìš©ëŸ‰ ì›ë³¸ ë°ì´í„° ë°˜í™˜ (JSON)
export function getTokenUsageData(): DailyTokenUsage {
  ensureCurrentDay();
  return { ...dailyUsage };
}

// ============================================================
// Rate Limit / Quota ìë™ í´ë°± ì‹œìŠ¤í…œ + ìš”ì²­ í
// ============================================================

const COOLDOWN_MS = 60_000; // 60ì´ˆ ì¿¨ë‹¤ìš´

// í”„ë¡œë°”ì´ë”ë³„ ì¿¨ë‹¤ìš´ ìƒíƒœ
const providerCooldowns = new Map<ProviderType, number>();

// í”„ë¡œë°”ì´ë”ë³„ í´ë°± ìˆœì„œ
const FALLBACK_CHAIN: Record<ProviderType, ProviderType[]> = {
  gemini: ['openai', 'anthropic'],
  openai: ['anthropic', 'gemini'],
  anthropic: ['openai', 'gemini'],
};

// í”„ë¡œë°”ì´ë”ë³„ ê¸°ë³¸ ëª¨ë¸ (í´ë°± ì‹œ ì‚¬ìš©)
const DEFAULT_MODELS: Record<ProviderType, string> = {
  anthropic: config.CLAUDE_MODEL,
  openai: 'gpt-4o',
  gemini: 'gemini-2.0-flash',
};

// í”„ë¡œë°”ì´ë”ë³„ ê¸°ë³¸ ëª¨ë¸ ë°˜í™˜ (agentic í´ë°± ì‹œ ì‚¬ìš©)
export function getDefaultModel(provider: ProviderType): string {
  return DEFAULT_MODELS[provider];
}

// --- ìš”ì²­ í (í”„ë¡œë°”ì´ë”ë³„ ì§ë ¬í™”) ---
const providerQueues = new Map<ProviderType, Promise<unknown>>();

async function enqueue<T>(provider: ProviderType, fn: () => Promise<T>): Promise<T> {
  const prev = providerQueues.get(provider) ?? Promise.resolve();
  const next = prev.then(() => fn(), () => fn());
  providerQueues.set(provider, next);
  return next;
}

// --- ì—ëŸ¬ ë¶„ë¥˜ ---
export function isRateLimitError(err: unknown): boolean {
  const msg = getErrorMessage(err).toLowerCase();
  const status = getErrorStatus(err);
  return status === 429 || msg.includes('rate limit') || msg.includes('quota') || msg.includes('resource_exhausted');
}

function isBillingError(err: unknown): boolean {
  const msg = getErrorMessage(err).toLowerCase();
  const status = getErrorStatus(err);
  return status === 402 || msg.includes('billing') || msg.includes('credit') || msg.includes('payment')
    || msg.includes('insufficient') || msg.includes('exceeded');
}

function isAuthError(err: unknown): boolean {
  const msg = getErrorMessage(err).toLowerCase();
  const status = getErrorStatus(err);
  return status === 401 || status === 403 || msg.includes('auth') || msg.includes('api key')
    || msg.includes('permission') || msg.includes('forbidden');
}

function getErrorMessage(err: unknown): string {
  if (err instanceof Error) return err.message;
  return String(err);
}

function getErrorStatus(err: unknown): number {
  if (typeof err === 'object' && err !== null && 'status' in err) {
    return (err as { status: number }).status;
  }
  // Anthropic SDK error
  if (err instanceof Error && 'status' in err) {
    return (err as any).status;
  }
  return 0;
}

// ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ì—ëŸ¬ ìš”ì•½
export function getErrorSummary(err: unknown): string {
  if (isRateLimitError(err)) return 'â³ API ì‚¬ìš©ëŸ‰ ì´ˆê³¼ (Rate Limit)';
  if (isBillingError(err)) return 'ğŸ’³ ê²°ì œ/í¬ë ˆë”§ ë¬¸ì œ';
  if (isAuthError(err)) return 'ğŸ”‘ API í‚¤ ì¸ì¦ ì‹¤íŒ¨';
  const status = getErrorStatus(err);
  if (status >= 500) return `ğŸ”¥ ì„œë²„ ì˜¤ë¥˜ (${status})`;
  return `âŒ ${getErrorMessage(err).slice(0, 100)}`;
}

function isProviderAvailable(provider: ProviderType): boolean {
  if (provider === 'openai' && !config.OPENAI_API_KEY) return false;
  if (provider === 'gemini' && !config.GEMINI_API_KEY) return false;

  const cooldownUntil = providerCooldowns.get(provider);
  if (cooldownUntil && Date.now() < cooldownUntil) return false;

  if (cooldownUntil) {
    providerCooldowns.delete(provider);
    logger.info('PROVIDER', `${provider} ì¿¨ë‹¤ìš´ í•´ì œ â†’ ë‹¤ì‹œ ì‚¬ìš© ê°€ëŠ¥`);
  }
  return true;
}

function markProviderCooldown(provider: ProviderType) {
  const until = Date.now() + COOLDOWN_MS;
  providerCooldowns.set(provider, until);
  logger.warn('PROVIDER', `${provider} ì¿¨ë‹¤ìš´ ì„¤ì • (${COOLDOWN_MS / 1000}s) â†’ í´ë°± ì‹œë„`);
}

// --- ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ---
async function withRetry<T>(fn: () => Promise<T>, maxRetries: number = 3, baseDelay: number = 5000): Promise<T> {
  let lastErr: unknown;
  for (let i = 0; i <= maxRetries; i++) {
    try {
      return await fn();
    } catch (err) {
      lastErr = err;
      if (isRateLimitError(err) && i < maxRetries) {
        const delay = baseDelay * Math.pow(2, i);
        logger.warn('RETRY', `Rate limit â†’ ${delay / 1000}s ëŒ€ê¸° í›„ ì¬ì‹œë„ (${i + 1}/${maxRetries})`);
        await new Promise(r => setTimeout(r, delay));
        continue;
      }
      throw err;
    }
  }
  throw lastErr;
}

// ============================================================
// Provider clients (lazy init)
// ============================================================

let anthropicClient: Anthropic | null = null;
let openaiClient: OpenAI | null = null;
let geminiClient: GoogleGenerativeAI | null = null;

function getAnthropicClient(): Anthropic {
  if (!anthropicClient) {
    anthropicClient = new Anthropic({ apiKey: config.ANTHROPIC_API_KEY });
  }
  return anthropicClient;
}

function getOpenAIClient(): OpenAI {
  if (!openaiClient) {
    if (!config.OPENAI_API_KEY) throw new Error('OPENAI_API_KEY not set');
    openaiClient = new OpenAI({ apiKey: config.OPENAI_API_KEY });
  }
  return openaiClient;
}

function getGeminiClient(): GoogleGenerativeAI {
  if (!geminiClient) {
    if (!config.GEMINI_API_KEY) throw new Error('GEMINI_API_KEY not set');
    geminiClient = new GoogleGenerativeAI(config.GEMINI_API_KEY);
  }
  return geminiClient;
}

export { getOpenAIClient };

// ============================================================
// callLLM - ë„êµ¬ ì—†ì´ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì‘ë‹µ (ìë™ í´ë°± + í)
// mode='fast' â†’ POê°€ Haiku ëª¨ë¸ë¡œ ë¹ ë¥¸ ì‘ë‹µ
// ============================================================

export async function callLLM(
  agentType: AgentType,
  systemPrompt: string,
  messages: Array<{ role: 'user' | 'assistant'; content: string }>,
  mode?: 'fast' // PO ì „ìš©: Haiku ë¹ ë¥¸ ì‘ë‹µ ëª¨ë“œ
): Promise<string> {
  const mc = AGENT_MODELS[agentType];
  let primaryProvider = mc.provider;
  let primaryModel = mc.model;

  // PO fast ëª¨ë“œ: Haiku ëª¨ë¸ ì‚¬ìš© (Anthropic í•œì •)
  if (mode === 'fast' && agentType === 'po' && primaryProvider === 'anthropic') {
    primaryModel = PO_FAST_MODEL;
    logger.info('PO', `Fast mode â†’ ${primaryModel}`);
  }

  const candidates: Array<{ provider: ProviderType; model: string }> = [];

  if (isProviderAvailable(primaryProvider)) {
    candidates.push({ provider: primaryProvider, model: primaryModel });
  }
  for (const fallback of FALLBACK_CHAIN[primaryProvider]) {
    if (isProviderAvailable(fallback)) {
      candidates.push({ provider: fallback, model: DEFAULT_MODELS[fallback] });
    }
  }

  if (candidates.length === 0) {
    const soonest = [...providerCooldowns.entries()].sort((a, b) => a[1] - b[1])[0];
    if (soonest) {
      const waitMs = Math.max(0, soonest[1] - Date.now());
      logger.warn(agentType.toUpperCase(), `ëª¨ë“  í”„ë¡œë°”ì´ë” ì¿¨ë‹¤ìš´ â†’ ${Math.ceil(waitMs / 1000)}s ëŒ€ê¸°`);
      await new Promise(r => setTimeout(r, waitMs + 1000));
      providerCooldowns.delete(soonest[0]);
      candidates.push({ provider: soonest[0], model: DEFAULT_MODELS[soonest[0]] });
    } else {
      throw new Error(`ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë”ê°€ ì—†ìŠµë‹ˆë‹¤`);
    }
  }

  let lastError: unknown;

  for (const { provider, model } of candidates) {
    const isFallback = provider !== primaryProvider;
    if (isFallback) {
      logger.warn(agentType.toUpperCase(), `í´ë°±: ${primaryProvider} â†’ ${provider}/${model}`);
    } else {
      logger.info(agentType.toUpperCase(), `Using ${provider}/${model}`);
    }

    try {
      return await enqueue(provider, () => callProviderDirect(provider, model, systemPrompt, messages, agentType));
    } catch (err) {
      lastError = err;
      const summary = getErrorSummary(err);
      logger.warn(agentType.toUpperCase(), `${provider}/${model} ì‹¤íŒ¨ [${summary}] â†’ í´ë°± ì‹œë„`);
      // Rate limitë§Œ ì¿¨ë‹¤ìš´ (ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¦‰ì‹œ í´ë°±)
      if (isRateLimitError(err)) {
        markProviderCooldown(provider);
      }
      continue;
    }
  }

  throw lastError;
}

// í”„ë¡œë°”ì´ë” ì§ì ‘ í˜¸ì¶œ (í…ìŠ¤íŠ¸ë§Œ)
async function callProviderDirect(
  provider: ProviderType,
  model: string,
  systemPrompt: string,
  messages: Array<{ role: 'user' | 'assistant'; content: string }>,
  agentType?: string
): Promise<string> {
  switch (provider) {
    case 'anthropic':
      return callAnthropic(model, systemPrompt, messages, agentType);
    case 'openai':
      return callOpenAI(model, systemPrompt, messages, agentType);
    case 'gemini':
      return callGemini(model, systemPrompt, messages, agentType);
  }
}

// ============================================================
// Anthropic Tool Use agentic í˜¸ì¶œ
// ============================================================

export async function callAnthropicWithTools(
  model: string,
  systemPrompt: string,
  messages: Anthropic.MessageParam[],
  tools: Anthropic.Tool[],
  agentType?: string
): Promise<Anthropic.Message> {
  return enqueue('anthropic', () =>
    withRetry(async () => {
      const client = getAnthropicClient();
      const response = await client.messages.create({
        model,
        max_tokens: 2048,
        system: systemPrompt,
        messages,
        tools,
      });

      // í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
      if (response.usage) {
        trackTokenUsage(
          'anthropic', model, agentType || 'unknown',
          response.usage.input_tokens, response.usage.output_tokens
        );
      }

      return response;
    }, 3, 5000)
  );
}

// ============================================================
// OpenAI Function Calling (ë„êµ¬ ì‚¬ìš©)
// ============================================================

export interface OpenAIToolCall {
  id: string;
  name: string;
  arguments: Record<string, unknown>;
}

export interface OpenAIToolResponse {
  text: string | null;
  toolCalls: OpenAIToolCall[];
  finishReason: string;
}

export async function callOpenAIWithTools(
  model: string,
  systemPrompt: string,
  messages: OpenAI.Chat.ChatCompletionMessageParam[],
  tools: OpenAI.Chat.ChatCompletionTool[],
  agentType?: string,
  toolChoice?: 'auto' | 'required' | 'none'
): Promise<OpenAIToolResponse> {
  return enqueue('openai', () =>
    withRetry(async () => {
      const client = getOpenAIClient();
      const response = await client.chat.completions.create({
        model,
        max_tokens: 2048,
        messages: [
          { role: 'system', content: systemPrompt },
          ...messages,
        ],
        tools,
        tool_choice: toolChoice ?? 'auto',
        parallel_tool_calls: false,  // ë„êµ¬ í˜¸ì¶œ ìˆœì°¨ ì‹¤í–‰ (ì•ˆì •ì„±)
      });

      // í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
      if (response.usage) {
        trackTokenUsage(
          'openai', model, agentType || 'unknown',
          response.usage.prompt_tokens, response.usage.completion_tokens
        );
      }

      const choice = response.choices[0];
      const message = choice?.message;

      const toolCalls: OpenAIToolCall[] = (message?.tool_calls ?? [])
        .filter((tc): tc is OpenAI.Chat.ChatCompletionMessageToolCall & { type: 'function' } =>
          tc.type === 'function'
        )
        .map((tc) => {
          let args: Record<string, unknown> = {};
          try {
            args = JSON.parse(tc.function.arguments || '{}');
          } catch (e) {
            logger.warn('OPENAI', `Tool arguments JSON parse error for ${tc.function.name}: ${tc.function.arguments}`);
          }
          return { id: tc.id, name: tc.function.name, arguments: args };
        });

      return {
        text: message?.content ?? null,
        toolCalls,
        finishReason: choice?.finish_reason ?? 'stop',
      };
    }, 3, 5000)
  );
}

// ============================================================
// Gemini Function Calling (ë„êµ¬ ì‚¬ìš©)
// ============================================================

export interface GeminiToolCall {
  name: string;
  arguments: Record<string, unknown>;
}

export interface GeminiToolResponse {
  text: string | null;
  toolCalls: GeminiToolCall[];
}

export async function callGeminiWithTools(
  model: string,
  systemPrompt: string,
  chatHistory: Array<{ role: 'user' | 'model'; parts: any[] }>,
  lastMessage: string | any[],
  tools: any[],
  agentType?: string
): Promise<GeminiToolResponse> {
  return enqueue('gemini', () =>
    withRetry(async () => {
      const client = getGeminiClient();
      const genModel = client.getGenerativeModel({
        model,
        systemInstruction: systemPrompt,
        tools,
      });

      const chat = genModel.startChat({ history: chatHistory });
      const msgContent = typeof lastMessage === 'string' ? lastMessage : lastMessage;
      const result = await chat.sendMessage(msgContent);
      const response = result.response;

      // í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  (Gemini)
      const usageMeta = response.usageMetadata;
      if (usageMeta) {
        trackTokenUsage(
          'gemini', model, agentType || 'unknown',
          usageMeta.promptTokenCount ?? 0, usageMeta.candidatesTokenCount ?? 0
        );
      }

      const toolCalls: GeminiToolCall[] = [];
      let text: string | null = null;

      for (const candidate of response.candidates ?? []) {
        for (const part of candidate.content?.parts ?? []) {
          if ('functionCall' in part && part.functionCall) {
            toolCalls.push({
              name: part.functionCall.name,
              arguments: (part.functionCall.args as Record<string, unknown>) ?? {},
            });
          }
          if ('text' in part && part.text) {
            text = (text ?? '') + part.text;
          }
        }
      }

      return { text, toolCalls };
    }, 3, 5000)
  );
}

// ============================================================
// TTS (OpenAI TTS-1)
// ============================================================

export async function generateTTS(text: string): Promise<Buffer> {
  const client = getOpenAIClient();
  const response = await client.audio.speech.create({
    model: 'tts-1',
    voice: 'nova',
    input: text,
    response_format: 'opus',
  });
  const arrayBuffer = await response.arrayBuffer();
  return Buffer.from(arrayBuffer);
}

// ============================================================
// í”„ë¡œë°”ì´ë”ë³„ êµ¬í˜„ (í…ìŠ¤íŠ¸ ì „ìš© + í† í° ì¶”ì )
// ============================================================

async function callAnthropic(
  model: string,
  systemPrompt: string,
  messages: Array<{ role: 'user' | 'assistant'; content: string }>,
  agentType?: string
): Promise<string> {
  const client = getAnthropicClient();
  const response = await client.messages.create({
    model,
    max_tokens: 2048,
    system: systemPrompt,
    messages,
  });

  // í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
  if (response.usage) {
    trackTokenUsage(
      'anthropic', model, agentType || 'unknown',
      response.usage.input_tokens, response.usage.output_tokens
    );
  }

  return response.content
    .filter((block) => block.type === 'text')
    .map((block) => block.text)
    .join('\n');
}

async function callOpenAI(
  model: string,
  systemPrompt: string,
  messages: Array<{ role: 'user' | 'assistant'; content: string }>,
  agentType?: string
): Promise<string> {
  const client = getOpenAIClient();
  const response = await client.chat.completions.create({
    model,
    max_tokens: 2048,
    messages: [
      { role: 'system', content: systemPrompt },
      ...messages,
    ],
  });

  // í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
  if (response.usage) {
    trackTokenUsage(
      'openai', model, agentType || 'unknown',
      response.usage.prompt_tokens, response.usage.completion_tokens
    );
  }

  return response.choices[0]?.message?.content ?? '';
}

async function callGemini(
  model: string,
  systemPrompt: string,
  messages: Array<{ role: 'user' | 'assistant'; content: string }>,
  agentType?: string
): Promise<string> {
  const client = getGeminiClient();
  const genModel = client.getGenerativeModel({
    model,
    systemInstruction: systemPrompt,
  });

  const history = messages.slice(0, -1).map((msg) => ({
    role: msg.role === 'user' ? 'user' as const : 'model' as const,
    parts: [{ text: msg.content }],
  }));

  const lastMessage = messages[messages.length - 1];

  const chat = genModel.startChat({ history });
  const result = await chat.sendMessage(lastMessage.content);

  // í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  (Gemini)
  const usageMeta = result.response.usageMetadata;
  if (usageMeta) {
    trackTokenUsage(
      'gemini', model, agentType || 'unknown',
      usageMeta.promptTokenCount ?? 0, usageMeta.candidatesTokenCount ?? 0
    );
  }

  return result.response.text();
}
