import Anthropic from '@anthropic-ai/sdk';
import OpenAI from 'openai';
import { type AgentType, type AgentRole } from '../config.js';
import {
  callLLM,
  callAnthropicWithTools,
  callOpenAIWithTools,
  callGeminiWithTools,
  AGENT_MODELS,
  PO_FAST_MODEL,
  getErrorSummary,
  getDefaultModel,
  type ProviderType,
} from '../providers/index.js';
import {
  executeTool,
  getOpenAITools,
  getGeminiTools,
  getToolsForAgent,
} from '../tools/index.js';
import { logger } from '../utils/logger.js';
import { ROLE_SYSTEM_PROMPTS, MANDATORY_RULES } from './role-prompts.js';
import { DEFAULT_BOT_ROLES, ROLE_PERMISSIONS, ROLE_DISPLAY_NAMES } from '../config/roles.js';

// ============================================================
// ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Hub-Spoke ëª¨ë¸)
// ============================================================

// ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ê²½ë¡œ
const WORKSPACE_BASE = 'D:\\\\projects\\\\miraclro\\\\multi-agent-bot\\\\workspace';

// PO tool description
const PO_TOOL_DESCRIPTION = `
${MANDATORY_RULES}

Running on server. Available tools:
- run_command: Shell (PowerShell). npm, git, python available
- read_file / write_file (auto-creates dirs) / list_directory
- http_request / system_info (CPU, RAM, GPU, disk, OS)
- platform_activity: ì™¸ë¶€ AI ì»¤ë®¤ë‹ˆí‹°(Moltbook/ë¨¸ìŠ´ë‹·ì»´) í™œë™ ì¡°íšŒ/íŠ¸ë¦¬ê±° (status, insights, trigger_cycle)
- dispatch_to_agent: Assign tasks to team â†’ receive results (dev/design/cs/marketing)

Project root: D:\\projects | Workspace: ${WORKSPACE_BASE}
PO output: ${WORKSPACE_BASE}\\po\\{project}\\ | Registry: ${WORKSPACE_BASE}\\shared\\projects.json | Shared: ${WORKSPACE_BASE}\\shared\\

PM rules:
- New project â†’ register in projects.json + create folder + dispatch first tasks
- Always tell workers: "save deliverables via write_file"
- Status check â†’ read projects.json + check team folders â†’ synthesize
- Project changes â†’ update projects.json

You are the HUB of 5-Bot architecture. All bots share one group.
Decompose tasks â†’ dispatch_to_agent â†’ synthesize results â†’ report to user.
Simple questions: answer directly. Specialized tasks: delegate to team.`;

// Worker tool description
const WORKER_TOOL_DESCRIPTION = `
${MANDATORY_RULES}

Running on server. Available tools:
- run_command (PowerShell: npm, git, python)
- read_file / write_file (auto-creates dirs) / list_directory
- http_request
- report_to_po: Report to PO (questions, escalation, collaboration requests)

Project root: D:\\projects | Workspace: ${WORKSPACE_BASE}

ACTION RULES:
1. Execute immediately with tools. Text-only = failure.
2. Order: list_directory â†’ read_file â†’ DO work â†’ write_file â†’ report
3. ALL deliverables MUST be saved via write_file.
4. "I will..." is FORBIDDEN. Do it NOW.
5. Need collaboration? â†’ report_to_po`;

export const SYSTEM_PROMPTS: Record<AgentType, string> = {
  po: `You are IRE (ì´ë ˆ) - PO (Product Owner) bot. Central orchestrator.
${PO_TOOL_DESCRIPTION}

Team (dispatch_to_agent targets):
- "dev" â†’ Daon: Software dev, code review, architecture, build/deploy
- "design" â†’ Chaea: UI/UX, wireframes, style guides, CSS
- "cs" â†’ Narae: Customer support, FAQ, VOC, tickets
- "marketing" â†’ Alli: Marketing strategy, content, SEO, market analysis

RULES:
1. First action: read_file("${WORKSPACE_BASE}\\\\shared\\\\projects.json")
2. "Do work" request â†’ read projects.json â†’ find pending â†’ dispatch_to_agent with specific tasks
3. dispatch MUST include: deliverable + save path + completion criteria
4. Never text-only. Always call tools. "I will..." FORBIDDEN.

RESPOND TO USER IN KOREAN (í•œêµ­ì–´). Be concise and practical.`,

  dev: `You are Daon (ë‹¤ì˜¨) - Dev bot. Software engineering specialist.
${WORKER_TOOL_DESCRIPTION}

Workspace: ${WORKSPACE_BASE}\\dev\\

Role: Code generation/review/refactoring, architecture, debugging, testing (npm test, pytest), git ops, build/deploy.
All deliverables (code, design docs, analysis) MUST be saved to workspace via write_file.

RESPOND TO USER IN KOREAN (í•œêµ­ì–´).`,

  design: `You are Chaea (ì±„ì•„) - Design bot. UI/UX design specialist.
${WORKER_TOOL_DESCRIPTION}

Workspace: ${WORKSPACE_BASE}\\design\\

Role: UI/UX direction, wireframes, design systems, style guides, HTML/CSS implementation, asset management, UX improvement, a11y.
All deliverables (mockups, CSS, wireframes) MUST be saved via write_file.

RESPOND TO USER IN KOREAN (í•œêµ­ì–´).`,

  cs: `You are Narae (ë‚˜ë˜) - CS bot. Customer support specialist.
${WORKER_TOOL_DESCRIPTION}

Workspace: ${WORKSPACE_BASE}\\cs\\

Role: Ticket classification, FAQ management, VOC reports, escalation via report_to_po, customer response scenarios, data analysis.
All deliverables (FAQ, VOC reports, scenarios) MUST be saved via write_file.
Be friendly and empathetic. RESPOND TO USER IN KOREAN (í•œêµ­ì–´).`,

  marketing: `You are Alli (ì•Œë¦¬) - Marketing bot. Marketing specialist.
${WORKER_TOOL_DESCRIPTION}

Workspace: ${WORKSPACE_BASE}\\marketing\\

Role: Content creation (SNS, blog, email, ad copy), campaign strategy, market analysis, SEO, competitor analysis, performance metrics.
All deliverables (content, strategy docs, reports) MUST be saved via write_file.

RESPOND TO USER IN KOREAN (í•œêµ­ì–´).`,
};

// ============================================================
// Agentic Agent - Hub-Spoke ëª¨ë¸ ê¸°ë°˜ í†µí•© í´ë˜ìŠ¤
// ============================================================

export class BaseAgent {
  readonly type: AgentType;
  currentRole: AgentRole;
  private conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }> = [];

  // í˜„ì¬ ìœ ì € ëŒ€í™”ê°€ ì§„í–‰ ì¤‘ì¸ ì±„íŒ… ID (ë´‡ ê°„ ëŒ€í™”ë¥¼ ì´ ì±„íŒ…ì— í‘œì‹œ)
  currentChatId?: string;

  // Anthropic ì „ìš© íˆìŠ¤í† ë¦¬ (tool use í¬í•¨)
  private anthropicHistory: Anthropic.MessageParam[] = [];
  // OpenAI ì „ìš© íˆìŠ¤í† ë¦¬
  private openaiHistory: OpenAI.Chat.ChatCompletionMessageParam[] = [];
  // Gemini ì „ìš© íˆìŠ¤í† ë¦¬
  private geminiHistory: Array<{ role: 'user' | 'model'; parts: any[] }> = [];

  constructor(type: AgentType) {
    this.type = type;
    this.currentRole = DEFAULT_BOT_ROLES[type].activeRole;
  }

  // ì—­í•  ì „í™˜
  switchRole(newRole: AgentRole): boolean {
    const config = DEFAULT_BOT_ROLES[this.type];
    if (!config.availableRoles.includes(newRole)) {
      logger.warn(this.type.toUpperCase(), `ì—­í•  ì „í™˜ ì‹¤íŒ¨: ${newRole}ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ê°€ëŠ¥: ${config.availableRoles.join(', ')})`);
      return false;
    }
    const oldRole = this.currentRole;
    this.currentRole = newRole;
    config.activeRole = newRole;
    config.permissions = ROLE_PERMISSIONS[newRole];
    logger.info(this.type.toUpperCase(), `ì—­í•  ì „í™˜: ${ROLE_DISPLAY_NAMES[oldRole]} â†’ ${ROLE_DISPLAY_NAMES[newRole]}`);
    return true;
  }

  // í˜„ì¬ ì—­í• ì— ë§ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
  getSystemPrompt(): string {
    return ROLE_SYSTEM_PROMPTS[this.currentRole] ?? SYSTEM_PROMPTS[this.type];
  }

  getRoleDisplayName(): string {
    return ROLE_DISPLAY_NAMES[this.currentRole];
  }

  // ë©”ì‹œì§€ í‚¤ì›Œë“œ ê¸°ë°˜ ì„œë¸Œì—­í•  ìë™ ê°ì§€
  detectMode(message: string): AgentRole | null {
    const lower = message.toLowerCase();

    if (this.type === 'dev') {
      if (/\[architect\]|ì„¤ê³„|ì•„í‚¤í…ì²˜|êµ¬ì¡°ì„¤ê³„|db\s?ëª¨ë¸|api\s?ì„¤ê³„|erd/.test(lower)) return 'dev-architect';
      if (/\[build\]|êµ¬í˜„|ì‘ì„±|ì½”ë”©|ê°œë°œí•´|ë§Œë“¤ì–´|ë¹Œë“œ/.test(lower)) return 'dev-builder';
      if (/\[refactor\]|ë¦¬íŒ©í† ë§|ìµœì í™”|ì„±ëŠ¥ê°œì„ |êµ¬ì¡°\s?ê°œì„ |í´ë¦°/.test(lower)) return 'dev-refactor';
    }

    if (this.type === 'marketing') {
      if (/\[content\]|ì½˜í…ì¸ |ê¸€\s?ì‘ì„±|ì¹´í”¼|ë¸”ë¡œê·¸|sns/.test(lower)) return 'growth-content';
      if (/\[funnel\]|í¼ë„|ì „í™˜ìœ¨|cta|ëœë”©|ì˜¨ë³´ë”©/.test(lower)) return 'growth-funnel';
      if (/\[data\]|ë°ì´í„°\s?ë¶„ì„|ì‹œì¥\s?ì¡°ì‚¬|ê²½ìŸì‚¬|íŠ¸ë Œë“œ|ì§€í‘œ/.test(lower)) return 'growth-data';
    }

    if (this.type === 'po') {
      if (/\[audit\]|ê°ì‚¬|ë³´ì•ˆ|ì·¨ì•½ì |race\s?condition|ê¶Œí•œ\s?ë¶„ì„/.test(lower)) return 'auditor';
    }

    return null;
  }

  getModelInfo(): string {
    const mc = AGENT_MODELS[this.type];
    if (this.type === 'po') {
      return `${mc.provider}/${mc.model} (fast: ${PO_FAST_MODEL})`;
    }
    return `${mc.provider}/${mc.model}`;
  }

  getProvider(): ProviderType {
    return AGENT_MODELS[this.type].provider;
  }

  // dispatch_to_agent / report_to_poì—ì„œ í˜¸ì¶œ - ë„êµ¬ ì—†ì´ ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì‘ë‹µ (ìˆœí™˜ ë°©ì§€)
  // PO: Haikuë¡œ ë¹ ë¥¸ ì‘ë‹µ (ì‚¬ìš©ì ì§ˆë¬¸) / Sonnetìœ¼ë¡œ ë¶„ì„ (íŒ€ì› ë³´ê³ )
  async handleDirectMessage(message: string, senderName: string): Promise<string> {
    this.conversationHistory.push({
      role: 'user',
      content: `[${senderName}] ${message}`,
    });

    if (this.conversationHistory.length > 10) {
      this.conversationHistory = this.conversationHistory.slice(-10);
    }

    try {
      // PO ë“€ì–¼ ëª¨ë¸: íŒ€ì› ë³´ê³  â†’ Sonnet(ê¸°ë³¸), ì‚¬ìš©ì/ì‹œìŠ¤í…œ â†’ Haiku(ë¹ ë¥¸)
      const isFromWorker = ['ë‹¤ì˜¨', 'ì±„ì•„', 'ë‚˜ë˜', 'ì•Œë¦¬'].some(name => senderName.includes(name));
      const useModel = (this.type === 'po' && !isFromWorker) ? 'fast' : 'default';

      const assistantMessage = await callLLM(
        this.type,
        this.getSystemPrompt(),
        this.conversationHistory,
        useModel === 'fast' ? 'fast' : undefined
      );

      this.conversationHistory.push({
        role: 'assistant',
        content: assistantMessage,
      });

      logger.info(this.type.toUpperCase(), `Direct response [${useModel}] (${assistantMessage.length} chars)`);
      return assistantMessage;
    } catch (err) {
      logger.error(this.type.toUpperCase(), `Direct message error`, err);
      throw err;
    }
  }

  // ë©”ì¸ ë©”ì‹œì§€ ì²˜ë¦¬ - í”„ë¡œë°”ì´ë”ì— ë”°ë¼ ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥í•œ agentic ë£¨í”„
  // PO: Sonnet ì‚¬ìš© (ë¶„ì„/ì¡°ìœ¨ ëª¨ë“œ)
  // Worker: ì„¤ì •ëœ í”„ë¡œë°”ì´ë”/ëª¨ë¸ ì‚¬ìš©
  async handleMessage(message: string, senderName: string): Promise<string> {
    // ìë™ ëª¨ë“œ ê°ì§€
    const detectedRole = this.detectMode(message);
    if (detectedRole && detectedRole !== this.currentRole) {
      this.switchRole(detectedRole);
    }

    const provider = this.getProvider();

    // Agentic ëª¨ë“œ í´ë°± ìˆœì„œ: ì£¼ í”„ë¡œë°”ì´ë” â†’ ë‹¤ë¥¸ í”„ë¡œë°”ì´ë” Agentic â†’ í…ìŠ¤íŠ¸ ëª¨ë“œ
    const AGENTIC_FALLBACK: Record<ProviderType, ProviderType[]> = {
      anthropic: ['openai', 'gemini'],
      openai: ['anthropic', 'gemini'],
      gemini: ['openai', 'anthropic'],
    };

    // í´ë°± ì‹œ í•´ë‹¹ í”„ë¡œë°”ì´ë”ì˜ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš© (e.g., claude-opus-4-6ë¥¼ OpenAIì— ë³´ë‚´ì§€ ì•Šë„ë¡)
    const getModelForProvider = (p: ProviderType): string => {
      if (p === provider) return AGENT_MODELS[this.type].model; // ì£¼ í”„ë¡œë°”ì´ë” â†’ ì›ë˜ ëª¨ë¸
      return getDefaultModel(p); // í´ë°± í”„ë¡œë°”ì´ë” â†’ í•´ë‹¹ í”„ë¡œë°”ì´ë” ê¸°ë³¸ ëª¨ë¸
    };

    const tryAgentic = async (p: ProviderType): Promise<string> => {
      const model = getModelForProvider(p);
      switch (p) {
        case 'anthropic': return await this.handleMessageAnthropic(message, senderName, model);
        case 'openai': return await this.handleMessageOpenAI(message, senderName, model);
        case 'gemini': return await this.handleMessageGemini(message, senderName, model);
        default: throw new Error(`Unknown provider: ${p}`);
      }
    };

    // 1ì°¨: ì£¼ í”„ë¡œë°”ì´ë” Agentic
    try {
      return await tryAgentic(provider);
    } catch (err) {
      logger.warn(this.type.toUpperCase(), `Agentic ì‹¤íŒ¨ (${provider}): ${err instanceof Error ? err.message : err}`);
    }

    // 2ì°¨: í´ë°± í”„ë¡œë°”ì´ë”ë“¤ì˜ Agentic ëª¨ë“œ (ë„êµ¬ ì‚¬ìš© ìœ ì§€!)
    const fallbacks = AGENTIC_FALLBACK[provider] ?? [];
    for (const fb of fallbacks) {
      try {
        logger.info(this.type.toUpperCase(), `Agentic í´ë°± ì‹œë„: ${provider} â†’ ${fb} (model: ${getModelForProvider(fb)})`);
        return await tryAgentic(fb);
      } catch (fbErr) {
        logger.warn(this.type.toUpperCase(), `Agentic í´ë°± ì‹¤íŒ¨ (${fb}): ${fbErr instanceof Error ? fbErr.message : fbErr}`);
      }
    }

    // 3ì°¨: ìµœí›„ ìˆ˜ë‹¨ - í…ìŠ¤íŠ¸ ëª¨ë“œ (ë„êµ¬ ì—†ìŒ)
    logger.error(this.type.toUpperCase(), `ëª¨ë“  Agentic í”„ë¡œë°”ì´ë” ì‹¤íŒ¨ â†’ í…ìŠ¤íŠ¸ ëª¨ë“œ í´ë°±`);
    try {
      return await this.handleDirectMessage(message, senderName);
    } catch (fallbackErr) {
      const summary = getErrorSummary(fallbackErr);
      logger.error(this.type.toUpperCase(), `í…ìŠ¤íŠ¸ ëª¨ë“œë„ ì‹¤íŒ¨: ${summary}`, fallbackErr);
      return `âš ï¸ AI ì„œë¹„ìŠ¤ ì˜¤ë¥˜: ${summary}\nëª¨ë“  í”„ë¡œë°”ì´ë”(Anthropic/OpenAI/Gemini)ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.`;
    }
  }

  async handleTask(taskDescription: string): Promise<string> {
    return this.handleMessage(`[ì‘ì—… ì§€ì‹œ] ${taskDescription}`, 'System');
  }

  clearHistory() {
    this.conversationHistory = [];
    this.anthropicHistory = [];
    this.openaiHistory = [];
    this.geminiHistory = [];
  }

  // ============================================================
  // Anthropic Agentic Loop (Claude Tool Use)
  // ============================================================

  private async handleMessageAnthropic(message: string, senderName: string, modelOverride?: string): Promise<string> {
    const historySnapshot = this.anthropicHistory.length;

    this.anthropicHistory.push({
      role: 'user',
      content: `[${senderName}] ${message}`,
    });

    if (this.anthropicHistory.length > 16) {
      this.anthropicHistory = this.anthropicHistory.slice(-16);
    }

    const model = modelOverride ?? AGENT_MODELS[this.type].model;
    // ì—­í• ë³„ ë„êµ¬ ì„¸íŠ¸: PO â†’ dispatch_to_agent, Worker â†’ report_to_po
    const tools = getToolsForAgent(this.type) as Anthropic.Tool[];

    try {
      let response = await callAnthropicWithTools(
        model,
        this.getSystemPrompt(),
        this.anthropicHistory,
        tools,
        this.type
      );

      let iterations = 0;
      const MAX_ITERATIONS = 7;
      let writeFileCalled = false;

      while (response.stop_reason === 'tool_use' && iterations < MAX_ITERATIONS) {
        iterations++;

        this.anthropicHistory.push({
          role: 'assistant',
          content: response.content,
        });

        const toolResults: Anthropic.ToolResultBlockParam[] = [];
        for (const block of response.content) {
          if (block.type === 'tool_use') {
            logger.info(this.type.toUpperCase(), `Tool call: ${block.name} (iteration ${iterations})`);
            if (block.name === 'write_file') writeFileCalled = true;
            const result = await executeTool(
              block.name,
              block.input as Record<string, unknown>,
              this.type
            );
            toolResults.push({
              type: 'tool_result',
              tool_use_id: block.id,
              content: result,
            });
          }
        }

        this.anthropicHistory.push({
          role: 'user',
          content: toolResults,
        });

        response = await callAnthropicWithTools(
          model,
          this.getSystemPrompt(),
          this.anthropicHistory,
          tools,
          this.type
        );
      }

      // ğŸš¨ write_file ë¯¸í˜¸ì¶œ ê°ì§€ â†’ ê°•ì œ ì¬ì‹œë„ (PO ì œì™¸)
      if (!writeFileCalled && this.type !== 'po' && iterations > 0 && iterations < MAX_ITERATIONS) {
        logger.warn(this.type.toUpperCase(), `write_file ë¯¸í˜¸ì¶œ ê°ì§€! ê°•ì œ ì¬ì‹œë„`);

        this.anthropicHistory.push({
          role: 'assistant',
          content: response.content,
        });

        this.anthropicHistory.push({
          role: 'user',
          content: 'ğŸš¨ ì‘ì—… ë¯¸ì™„ë£Œ ê°ì§€: write_fileë¡œ ì‚°ì¶œë¬¼ì„ ì €ì¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì§€ê¸ˆ ì¦‰ì‹œ write_fileì„ í˜¸ì¶œí•˜ì—¬ ì‘ì—… ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì„¸ìš”. íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•Šìœ¼ë©´ ì—…ë¬´ ì‹¤íŒ¨ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.',
        });

        for (let retry = 0; retry < 3 && !writeFileCalled; retry++) {
          response = await callAnthropicWithTools(
            model,
            this.getSystemPrompt(),
            this.anthropicHistory,
            tools,
            this.type
          );

          if (response.stop_reason !== 'tool_use') break;

          iterations++;
          this.anthropicHistory.push({
            role: 'assistant',
            content: response.content,
          });

          const retryResults: Anthropic.ToolResultBlockParam[] = [];
          for (const block of response.content) {
            if (block.type === 'tool_use') {
              logger.info(this.type.toUpperCase(), `Retry tool call: ${block.name} (retry ${retry + 1})`);
              if (block.name === 'write_file') writeFileCalled = true;
              const result = await executeTool(
                block.name,
                block.input as Record<string, unknown>,
                this.type
              );
              retryResults.push({
                type: 'tool_result',
                tool_use_id: block.id,
                content: result,
              });
            }
          }

          this.anthropicHistory.push({
            role: 'user',
            content: retryResults,
          });
        }

        if (!writeFileCalled) {
          logger.error(this.type.toUpperCase(), `write_file ê°•ì œ ì¬ì‹œë„ ì‹¤íŒ¨ - ì‚°ì¶œë¬¼ ë¯¸ì €ì¥`);
        }

        // ë§ˆì§€ë§‰ í…ìŠ¤íŠ¸ ì‘ë‹µ
        response = await callAnthropicWithTools(
          model,
          this.getSystemPrompt(),
          this.anthropicHistory,
          tools,
          this.type
        );
      }

      const finalText = response.content
        .filter((block): block is Anthropic.TextBlock => block.type === 'text')
        .map((block) => block.text)
        .join('\n');

      this.anthropicHistory.push({
        role: 'assistant',
        content: response.content,
      });

      this.syncConversationHistory(message, senderName, finalText);

      logger.info(this.type.toUpperCase(), `Agentic response (${finalText.length} chars, ${iterations} tool calls, writeFile: ${writeFileCalled})`);
      return finalText;
    } catch (err) {
      this.anthropicHistory.length = historySnapshot;
      throw err;
    }
  }

  // ============================================================
  // OpenAI Agentic Loop (Function Calling)
  // ============================================================

  private async handleMessageOpenAI(message: string, senderName: string, modelOverride?: string): Promise<string> {
    const historySnapshot = this.openaiHistory.length;

    this.openaiHistory.push({
      role: 'user',
      content: `[${senderName}] ${message}`,
    });

    if (this.openaiHistory.length > 16) {
      this.openaiHistory = this.openaiHistory.slice(-16);
    }

    const model = modelOverride ?? AGENT_MODELS[this.type].model;
    // ì—­í• ë³„ ë„êµ¬ ì„¸íŠ¸
    const tools = getOpenAITools(getToolsForAgent(this.type));

    try {
      let iterations = 0;
      const MAX_ITERATIONS = 7;
      const FORCE_TOOL_ITERATIONS = 3; // ì²˜ìŒ 3íšŒëŠ” ë°˜ë“œì‹œ ë„êµ¬ í˜¸ì¶œ ê°•ì œ
      let writeFileCalled = false;

      // ì²« í˜¸ì¶œ: tool_choice='required'ë¡œ ë°˜ë“œì‹œ ë„êµ¬ ì‚¬ìš© ê°•ì œ
      let response = await callOpenAIWithTools(
        model,
        this.getSystemPrompt(),
        this.openaiHistory,
        tools,
        this.type,
        'required'
      );

      while (response.toolCalls.length > 0 && iterations < MAX_ITERATIONS) {
        iterations++;

        const assistantMsg: OpenAI.Chat.ChatCompletionAssistantMessageParam = {
          role: 'assistant',
          content: response.text || '',
          tool_calls: response.toolCalls.map((tc) => ({
            id: tc.id,
            type: 'function' as const,
            function: {
              name: tc.name,
              arguments: JSON.stringify(tc.arguments),
            },
          })),
        };
        this.openaiHistory.push(assistantMsg);

        for (const tc of response.toolCalls) {
          logger.info(this.type.toUpperCase(), `Tool call: ${tc.name} (iteration ${iterations})`);
          if (tc.name === 'write_file') writeFileCalled = true;
          const result = await executeTool(tc.name, tc.arguments, this.type);

          this.openaiHistory.push({
            role: 'tool',
            tool_call_id: tc.id,
            content: result,
          });
        }

        // ì²˜ìŒ 3íšŒëŠ” tool_choice='required', ì´í›„ 'auto'
        const toolChoice = iterations < FORCE_TOOL_ITERATIONS ? 'required' : 'auto';
        response = await callOpenAIWithTools(
          model,
          this.getSystemPrompt(),
          this.openaiHistory,
          tools,
          this.type,
          toolChoice
        );
      }

      // ğŸš¨ write_file ë¯¸í˜¸ì¶œ ê°ì§€ â†’ ê°•ì œ ì¬ì‹œë„ (PO ì œì™¸)
      if (!writeFileCalled && this.type !== 'po' && iterations > 0 && iterations < MAX_ITERATIONS) {
        logger.warn(this.type.toUpperCase(), `write_file ë¯¸í˜¸ì¶œ ê°ì§€! ê°•ì œ ì¬ì‹œë„ (${iterations} iterations ì‚¬ìš©)`);

        // ë¦¬ë§ˆì¸ë” ë©”ì‹œì§€ ì£¼ì…
        this.openaiHistory.push({
          role: 'user',
          content: '[SYSTEM] ğŸš¨ ì‘ì—… ë¯¸ì™„ë£Œ ê°ì§€: write_fileë¡œ ì‚°ì¶œë¬¼ì„ ì €ì¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì§€ê¸ˆ ì¦‰ì‹œ write_fileì„ í˜¸ì¶œí•˜ì—¬ ì‘ì—… ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì„¸ìš”. íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•Šìœ¼ë©´ ì—…ë¬´ ì‹¤íŒ¨ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.',
        });

        // write_file ê°•ì œ í˜¸ì¶œ (ìµœëŒ€ 3íšŒ ì¶”ê°€ ì‹œë„)
        for (let retry = 0; retry < 3 && !writeFileCalled; retry++) {
          response = await callOpenAIWithTools(
            model,
            this.getSystemPrompt(),
            this.openaiHistory,
            tools,
            this.type,
            'required'
          );

          if (response.toolCalls.length > 0) {
            iterations++;
            const assistantMsg: OpenAI.Chat.ChatCompletionAssistantMessageParam = {
              role: 'assistant',
              content: response.text || '',
              tool_calls: response.toolCalls.map((tc) => ({
                id: tc.id,
                type: 'function' as const,
                function: {
                  name: tc.name,
                  arguments: JSON.stringify(tc.arguments),
                },
              })),
            };
            this.openaiHistory.push(assistantMsg);

            for (const tc of response.toolCalls) {
              logger.info(this.type.toUpperCase(), `Retry tool call: ${tc.name} (retry ${retry + 1})`);
              if (tc.name === 'write_file') writeFileCalled = true;
              const result = await executeTool(tc.name, tc.arguments, this.type);
              this.openaiHistory.push({
                role: 'tool',
                tool_call_id: tc.id,
                content: result,
              });
            }
          } else {
            break;
          }
        }

        if (!writeFileCalled) {
          logger.error(this.type.toUpperCase(), `write_file ê°•ì œ ì¬ì‹œë„ ì‹¤íŒ¨ - ì‚°ì¶œë¬¼ ë¯¸ì €ì¥`);
        }

        // ë§ˆì§€ë§‰ í…ìŠ¤íŠ¸ ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°
        response = await callOpenAIWithTools(
          model,
          this.getSystemPrompt(),
          this.openaiHistory,
          tools,
          this.type,
          'auto'
        );
      }

      const finalText = response.text ?? '';

      this.openaiHistory.push({
        role: 'assistant',
        content: finalText,
      });

      this.syncConversationHistory(message, senderName, finalText);

      logger.info(this.type.toUpperCase(), `Agentic response (${finalText.length} chars, ${iterations} tool calls, writeFile: ${writeFileCalled})`);
      return finalText;
    } catch (err) {
      this.openaiHistory.length = historySnapshot;
      throw err;
    }
  }

  // ============================================================
  // Gemini Agentic Loop (Function Calling)
  // ============================================================

  private async handleMessageGemini(message: string, senderName: string, modelOverride?: string): Promise<string> {
    const historySnapshot = this.geminiHistory.length;
    const model = modelOverride ?? AGENT_MODELS[this.type].model;
    // ì—­í• ë³„ ë„êµ¬ ì„¸íŠ¸
    const tools = getGeminiTools(getToolsForAgent(this.type));
    const userMessage = `[${senderName}] ${message}`;

    try {
      let iterations = 0;
      const MAX_ITERATIONS = 7;
      let writeFileCalled = false;

      let response = await callGeminiWithTools(
        model,
        this.getSystemPrompt(),
        this.geminiHistory,
        userMessage,
        tools,
        this.type
      );

      this.geminiHistory.push({
        role: 'user',
        parts: [{ text: userMessage }],
      });

      while (response.toolCalls.length > 0 && iterations < MAX_ITERATIONS) {
        iterations++;

        const modelParts: any[] = [];
        if (response.text) {
          modelParts.push({ text: response.text });
        }
        for (const tc of response.toolCalls) {
          modelParts.push({
            functionCall: { name: tc.name, args: tc.arguments },
          });
        }
        this.geminiHistory.push({ role: 'model', parts: modelParts });

        const functionResponseParts: any[] = [];
        for (const tc of response.toolCalls) {
          logger.info(this.type.toUpperCase(), `Tool call: ${tc.name} (iteration ${iterations})`);
          if (tc.name === 'write_file') writeFileCalled = true;
          const result = await executeTool(tc.name, tc.arguments, this.type);
          functionResponseParts.push({
            functionResponse: { name: tc.name, response: { result } },
          });
        }

        response = await callGeminiWithTools(
          model,
          this.getSystemPrompt(),
          this.geminiHistory,
          functionResponseParts,
          tools,
          this.type
        );

        this.geminiHistory.push({ role: 'user', parts: functionResponseParts });
      }

      // ğŸš¨ write_file ë¯¸í˜¸ì¶œ ê°ì§€ â†’ ê°•ì œ ì¬ì‹œë„ (PO ì œì™¸)
      if (!writeFileCalled && this.type !== 'po' && iterations > 0 && iterations < MAX_ITERATIONS) {
        logger.warn(this.type.toUpperCase(), `write_file ë¯¸í˜¸ì¶œ ê°ì§€! ê°•ì œ ì¬ì‹œë„`);

        // ë¦¬ë§ˆì¸ë” ë©”ì‹œì§€ ì£¼ì…
        const reminderMessage = 'ğŸš¨ ì‘ì—… ë¯¸ì™„ë£Œ ê°ì§€: write_fileë¡œ ì‚°ì¶œë¬¼ì„ ì €ì¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì§€ê¸ˆ ì¦‰ì‹œ write_fileì„ í˜¸ì¶œí•˜ì—¬ ì‘ì—… ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì„¸ìš”. íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•Šìœ¼ë©´ ì—…ë¬´ ì‹¤íŒ¨ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.';

        for (let retry = 0; retry < 3 && !writeFileCalled; retry++) {
          // ë§ˆì§€ë§‰ í…ìŠ¤íŠ¸ ì‘ë‹µì´ ìˆìœ¼ë©´ ëª¨ë¸ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
          if (response.text) {
            this.geminiHistory.push({ role: 'model', parts: [{ text: response.text }] });
          }

          response = await callGeminiWithTools(
            model,
            this.getSystemPrompt(),
            this.geminiHistory,
            reminderMessage,
            tools,
            this.type
          );
          this.geminiHistory.push({ role: 'user', parts: [{ text: reminderMessage }] });

          if (response.toolCalls.length === 0) break;

          iterations++;
          const modelParts: any[] = [];
          if (response.text) modelParts.push({ text: response.text });
          for (const tc of response.toolCalls) {
            modelParts.push({ functionCall: { name: tc.name, args: tc.arguments } });
          }
          this.geminiHistory.push({ role: 'model', parts: modelParts });

          const retryParts: any[] = [];
          for (const tc of response.toolCalls) {
            logger.info(this.type.toUpperCase(), `Retry tool call: ${tc.name} (retry ${retry + 1})`);
            if (tc.name === 'write_file') writeFileCalled = true;
            const result = await executeTool(tc.name, tc.arguments, this.type);
            retryParts.push({ functionResponse: { name: tc.name, response: { result } } });
          }

          response = await callGeminiWithTools(
            model,
            this.getSystemPrompt(),
            this.geminiHistory,
            retryParts,
            tools,
            this.type
          );
          this.geminiHistory.push({ role: 'user', parts: retryParts });
        }

        if (!writeFileCalled) {
          logger.error(this.type.toUpperCase(), `write_file ê°•ì œ ì¬ì‹œë„ ì‹¤íŒ¨ - ì‚°ì¶œë¬¼ ë¯¸ì €ì¥`);
        }
      }

      const finalText = response.text ?? '';

      this.geminiHistory.push({
        role: 'model',
        parts: [{ text: finalText || '(ì‘ë‹µ ì—†ìŒ)' }],
      });

      if (this.geminiHistory.length > 16) {
        this.geminiHistory = this.geminiHistory.slice(-16);
      }

      this.syncConversationHistory(message, senderName, finalText);

      logger.info(this.type.toUpperCase(), `Agentic response (${finalText.length} chars, ${iterations} tool calls, writeFile: ${writeFileCalled})`);
      return finalText;
    } catch (err) {
      this.geminiHistory.length = historySnapshot;
      throw err;
    }
  }

  // ì¼ë°˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë™ê¸°í™”
  private syncConversationHistory(message: string, senderName: string, response: string) {
    this.conversationHistory.push(
      { role: 'user', content: `[${senderName}] ${message}` },
      { role: 'assistant', content: response }
    );
    if (this.conversationHistory.length > 10) {
      this.conversationHistory = this.conversationHistory.slice(-10);
    }
  }
}

// POAgentëŠ” ì´ì œ BaseAgentì™€ ë™ì¼ (í•˜ìœ„ í˜¸í™˜ìš© ë³„ì¹­)
export class POAgent extends BaseAgent {
  constructor() {
    super('po');
  }
}

// ============================================================
// ì—ì´ì „íŠ¸ ì‹±ê¸€í†¤ ê´€ë¦¬
// ============================================================

const agents = new Map<AgentType, BaseAgent>();

export function getAgent(type: AgentType): BaseAgent {
  let agent = agents.get(type);
  if (!agent) {
    agent = type === 'po' ? new POAgent() : new BaseAgent(type);
    agents.set(type, agent);
  }
  return agent;
}

export function getAllAgents(): Map<AgentType, BaseAgent> {
  const types: AgentType[] = ['po', 'dev', 'design', 'cs', 'marketing'];
  for (const type of types) {
    const agent = getAgent(type);
    const role = type === 'po' ? 'HUB' : 'SPOKE';
    logger.info('AGENTS', `  ${type.toUpperCase()} [${role}] ${agent.getRoleDisplayName()} â†’ ${agent.getModelInfo()}`);
  }
  return agents;
}
